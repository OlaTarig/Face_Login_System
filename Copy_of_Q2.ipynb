{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE A COPY FIRST!"
      ],
      "metadata": {
        "id": "-r7PP3X3JvaQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OoLaW7vo3eC"
      },
      "source": [
        "## **Question 2: Arabic Handwritten Character Search**\n",
        "\n",
        "Ali‚Äôs younger brother is learning to write Arabic characters. One day, he wrote a character that looks like **\"ÿ®\"**, and Ali wants to find **the most similar 5 images** from a dataset of handwritten Arabic characters.\n",
        "\n",
        "üìù **Your Task:**  \n",
        "Complete the missing code cells below by implementing **image search** to find the most similar 5 images to the given input character.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43-3OiiSJurY"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF8OOwmTw7MC"
      },
      "source": [
        "### Query Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T06:30:04.904637Z",
          "iopub.status.busy": "2025-02-18T06:30:04.904181Z",
          "iopub.status.idle": "2025-02-18T06:30:04.909035Z",
          "shell.execute_reply": "2025-02-18T06:30:04.907557Z",
          "shell.execute_reply.started": "2025-02-18T06:30:04.904603Z"
        },
        "id": "FQPjN6xPvssH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Please uncomment this out when you are running this lab on google colab!\n",
        "import os\n",
        "\n",
        "# # Set KaggleHub cache to a directory inside /content/\n",
        "os.environ[\"KAGGLEHUB_CACHE\"] = \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zb3_G3UxudPV",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cd4d31d4-981b-42f5-ef7c-3638cf27d155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mohammad2012191/character?dataset_version_number=5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 320/320 [00:00<00:00, 551kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=32x32>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAeElEQVR4Ae2QSxaAIAhFqdP+t2w8UPlINXSSg4K4Xl8S/Wv/DTSJcD4GaSTEZQD3h3WjMgN4tY6ZvA0In7lhG/gAFCdE4D0D64oIzgB/QbjfZABEChLaLpguFAFIZ8gsAR7R0QL0JFMdLgpDLL+rBBTT5yfg4V31DZsUDx4RuZwmAAAAAElFTkSuQmCC\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiu01H4XeI9M8B2/jC4W0/s+ZI5DGs2ZY45DhHYYxg5XgEkbhkDBxxddp4s8Dw6B4O8L+IbK+kvoNWtybl1jHlwTcHZuBIzglcHnMTn2Xi69s0/wAQ+FPiJ4D0jwvretyeHta063FrFdupW2liBB2N8+1gVhiJ3lfnC7c9Dj6l8APGlrrK2llHaX1m7gLerOsaopYjLox3AgYYhQ3XALGrnxIe38LfC/QPh5dXkFzrtjdtd3ItCWjiRjKVBYgEMRKpxjoCTgFc+P0V2nw2+IM3w81m7vVs5L6C5t/Ke2FyYlLbgVc/KwJADAcfxnn14uv/2Q==\n"
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import kagglehub\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "path = kagglehub.dataset_download(\"mohammad2012191/character\")\n",
        "raw_query_image = Image.open(os.path.join(path,\"char.png\"))\n",
        "raw_query_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQzaaxRWw9I6"
      },
      "source": [
        "### Arabic Characters Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M83HESLawt9N",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b504a5-1141-42d5-b4b3-68097499f133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mohammad2012191/arabic-chars?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219k/219k [00:00<00:00, 53.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /content/data/datasets/mohammad2012191/arabic-chars/versions/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "dataset_path = kagglehub.dataset_download(\"mohammad2012191/arabic-chars\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Just Exploring the dataset\n",
        "#It seems that the labels are the order of the character in the alphabet which is part of the image name\n",
        "from PIL import Image\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "\n",
        "    print(f\"Current Directory: {root}\")\n",
        "\n",
        "    print(f\"Subdirectories: {dirs}\")\n",
        "\n",
        "    print(f\"Files: {files}\")\n",
        "    image =  Image.open(\"/content/data/datasets/mohammad2012191/arabic-chars/versions/3/chars images/id_1989_label_15.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9d14G5nWfh4",
        "outputId": "f92fcc69-5f51-44aa-b4c9-2ead0974d363"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content/data/datasets/mohammad2012191/arabic-chars/versions/3\n",
            "Subdirectories: ['chars images']\n",
            "Files: []\n",
            "Current Directory: /content/data/datasets/mohammad2012191/arabic-chars/versions/3/chars images\n",
            "Subdirectories: []\n",
            "Files: ['id_1989_label_15.png', 'id_493_label_23.png', 'id_2095_label_12.png', 'id_2642_label_5.png', 'id_270_label_23.png', 'id_3186_label_25.png', 'id_294_label_7.png', 'id_1875_label_14.png', 'id_2385_label_17.png', 'id_2902_label_23.png', 'id_3157_label_11.png', 'id_2719_label_16.png', 'id_2153_label_13.png', 'id_2309_label_7.png', 'id_2981_label_7.png', 'id_864_label_12.png', 'id_3047_label_12.png', 'id_919_label_12.png', 'id_527_label_12.png', 'id_1607_label_20.png', 'id_347_label_6.png', 'id_13229_label_2.png', 'id_2240_label_28.png', 'id_1014_label_3.png', 'id_2114_label_21.png', 'id_1339_label_26.png', 'id_1366_label_11.png', 'id_1433_label_17.png', 'id_340_label_2.png', 'id_890_label_25.png', 'id_2477_label_2.png', 'id_977_label_13.png', 'id_1925_label_11.png', 'id_2717_label_15.png', 'id_1640_label_8.png', 'id_988_label_18.png', 'id_3303_label_28.png', 'id_1712_label_16.png', 'id_2973_label_3.png', 'id_2049_label_17.png', 'id_1483_label_14.png', 'id_1789_label_27.png', 'id_1136_label_2.png', 'id_3113_label_17.png', 'id_1129_label_5.png', 'id_352_label_8.png', 'id_2519_label_28.png', 'id_2574_label_27.png', 'id_337_label_1.png', 'id_1268_label_18.png', 'id_1507_label_26.png', 'id_2507_label_22.png', 'id_1743_label_4.png', 'id_13231_label_2.png', 'id_2301_label_3.png', 'id_2056_label_20.png', 'id_2027_label_2.png', 'id_460_label_6.png', 'id_1629_label_3.png', 'id_3206_label_7.png', 'id_2540_label_10.png', 'id_826_label_21.png', 'id_369_label_17.png', 'id_3107_label_14.png', 'id_1604_label_18.png', 'id_2671_label_20.png', 'id_1657_label_17.png', 'id_2349_label_27.png', 'id_315_label_18.png', 'id_2851_label_26.png', 'id_2072_label_28.png', 'id_13006_label_2.png', 'id_2028_label_2.png', 'id_971_label_10.png', 'id_15_label_2.png', 'id_3082_label_1.png', 'id_303_label_12.png', 'id_3172_label_18.png', 'id_2545_label_13.png', 'id_1774_label_19.png', 'id_2928_label_2.png', 'id_2650_label_9.png', 'id_392_label_28.png', 'id_246_label_11.png', 'id_1324_label_18.png', 'id_1090_label_13.png', 'id_2160_label_16.png', 'id_1584_label_8.png', 'id_1524_label_6.png', 'id_2777_label_17.png', 'id_112_label_28.png', 'id_3330_label_13.png', 'id_833_label_25.png', 'id_911_label_8.png', 'id_1940_label_18.png', 'id_1101_label_19.png', 'id_3285_label_19.png', 'id_3014_label_23.png', 'id_1561_label_25.png', 'id_1341_label_27.png', 'id_483_label_18.png', 'id_1820_label_14.png', 'id_85_label_15.png', 'id_703_label_16.png', 'id_3349_label_23.png', 'id_288_label_4.png', 'id_2753_label_5.png', 'id_1131_label_2.png', 'id_3269_label_11.png', 'id_1806_label_7.png', 'id_966_label_7.png', 'id_2214_label_15.png', 'id_3055_label_16.png', 'id_1751_label_8.png', 'id_2730_label_21.png', 'id_1860_label_6.png', 'id_2899_label_22.png', 'id_1237_label_3.png', 'id_1418_label_9.png', 'id_3176_label_20.png', 'id_3087_label_4.png', 'id_1970_label_5.png', 'id_11_label_2.png', 'id_168_label_28.png', 'id_2453_label_23.png', 'id_1353_label_5.png', 'id_1663_label_20.png', 'id_899_label_2.png', 'id_466_label_9.png', 'id_1295_label_4.png', 'id_1059_label_26.png', 'id_982_label_15.png', 'id_983_label_16.png', 'id_1647_label_12.png', 'id_3034_label_5.png', 'id_2503_label_20.png', 'id_2595_label_10.png', 'id_1060_label_26.png', 'id_1174_label_27.png', 'id_928_label_16.png', 'id_2234_label_25.png', 'id_1926_label_11.png', 'id_1441_label_21.png', 'id_2756_label_6.png', 'id_1711_label_16.png', 'id_783_label_28.png', 'id_1386_label_21.png', 'id_3050_label_13.png', 'id_1576_label_4.png', 'id_1404_label_2.png', 'id_30_label_15.png', 'id_506_label_1.png', 'id_2737_label_25.png', 'id_244_label_10.png', 'id_2921_label_5.png', 'id_575_label_8.png', 'id_1029_label_11.png', 'id_3146_label_5.png', 'id_46_label_23.png', 'id_144_label_16.png', 'id_165_label_27.png', 'id_135_label_12.png', 'id_2287_label_24.png', 'id_934_label_19.png', 'id_3231_label_20.png', 'id_2897_label_21.png', 'id_1973_label_7.png', 'id_819_label_18.png', 'id_2010_label_25.png', 'id_32_label_16.png', 'id_663_label_24.png', 'id_1135_label_2.png', 'id_2885_label_15.png', 'id_2026_label_2.png', 'id_2321_label_13.png', 'id_1211_label_18.png', 'id_281_label_1.png', 'id_1510_label_27.png', 'id_774_label_23.png', 'id_475_label_14.png', 'id_1884_label_18.png', 'id_2041_label_13.png', 'id_1043_label_18.png', 'id_13230_label_2.png', 'id_1454_label_27.png', 'id_1861_label_7.png', 'id_82_label_13.png', 'id_2520_label_28.png', 'id_2005_label_23.png', 'id_675_label_2.png', 'id_863_label_12.png', 'id_1578_label_5.png', 'id_101_label_23.png', 'id_397_label_3.png', 'id_815_label_16.png', 'id_3015_label_24.png', 'id_1531_label_10.png', 'id_2040_label_12.png', 'id_1462_label_3.png', 'id_3287_label_20.png', 'id_193_label_13.png', 'id_1196_label_10.png', 'id_1736_label_28.png', 'id_1818_label_13.png', 'id_14_label_2.png', 'id_3149_label_2.png', 'id_1844_label_26.png', 'id_2388_label_18.png', 'id_2012_label_26.png', 'id_498_label_25.png', 'id_13004_label_2.png', 'id_1220_label_22.png', 'id_736_label_4.png', 'id_412_label_10.png', 'id_2747_label_2.png', 'id_699_label_14.png', 'id_13228_label_2.png', 'id_2752_label_4.png', 'id_874_label_17.png', 'id_3301_label_27.png', 'id_305_label_13.png', 'id_953_label_1.png', 'id_3093_label_7.png', 'id_428_label_18.png', 'id_2658_label_13.png', 'id_2560_label_20.png', 'id_3251_label_2.png', 'id_502_label_27.png', 'id_676_label_2.png', 'id_2194_label_5.png', 'id_2478_label_2.png', 'id_2923_label_2.png', 'id_40_label_20.png', 'id_2530_label_5.png', 'id_13002_label_2.png', 'id_116_label_2.png', 'id_13001_label_2.png', 'id_3056_label_16.png', 'id_3148_label_6.png', 'id_3340_label_18.png', 'id_952_label_28.png', 'id_632_label_8.png', 'id_1605_label_19.png', 'id_1570_label_1.png', 'id_884_label_22.png', 'id_2484_label_10.png', 'id_1763_label_14.png', 'id_1488_label_16.png', 'id_3151_label_2.png', 'id_1355_label_6.png', 'id_13005_label_2.png', 'id_364_label_14.png', 'id_2942_label_15.png', 'id_134_label_11.png', 'id_1133_label_2.png', 'id_417_label_13.png', 'id_875_label_18.png', 'id_2869_label_7.png', 'id_2766_label_11.png', 'id_1649_label_13.png', 'id_1764_label_14.png', 'id_797_label_7.png', 'id_2348_label_26.png', 'id_2482_label_9.png', 'id_752_label_12.png', 'id_2399_label_24.png', 'id_2606_label_15.png', 'id_1639_label_8.png', 'id_545_label_21.png', 'id_2769_label_13.png', 'id_2608_label_16.png', 'id_3252_label_2.png', 'id_1654_label_15.png', 'id_2960_label_24.png', 'id_3020_label_26.png', 'id_1563_label_26.png', 'id_1684_label_2.png', 'id_3268_label_10.png', 'id_2025_label_5.png', 'id_3137_label_1.png', 'id_1793_label_1.png', 'id_2842_label_21.png', 'id_1422_label_11.png', 'id_1722_label_21.png', 'id_1334_label_23.png', 'id_361_label_13.png', 'id_665_label_25.png', 'id_2925_label_2.png', 'id_1344_label_28.png', 'id_3152_label_2.png', 'id_1285_label_27.png', 'id_3333_label_15.png', 'id_503_label_28.png', 'id_478_label_15.png', 'id_2707_label_10.png', 'id_721_label_25.png', 'id_83_label_14.png', 'id_211_label_22.png', 'id_2016_label_28.png', 'id_302_label_11.png', 'id_1159_label_20.png', 'id_2725_label_19.png', 'id_930_label_17.png', 'id_2130_label_1.png', 'id_510_label_3.png', 'id_13008_label_2.png', 'id_3321_label_9.png', 'id_2911_label_28.png', 'id_2254_label_7.png', 'id_1247_label_8.png', 'id_1130_label_2.png', 'id_1476_label_10.png', 'id_2664_label_16.png', 'id_1627_label_2.png', 'id_1071_label_4.png', 'id_3238_label_23.png', 'id_2409_label_1.png', 'id_1894_label_23.png', 'id_247_label_12.png', 'id_2585_label_5.png', 'id_664_label_24.png', 'id_2676_label_22.png', 'id_1287_label_28.png', 'id_2531_label_6.png', 'id_618_label_1.png', 'id_3100_label_10.png', 'id_3131_label_26.png', 'id_2200_label_8.png', 'id_985_label_17.png', 'id_177_label_5.png', 'id_1886_label_19.png', 'id_838_label_27.png', 'id_994_label_21.png', 'id_1983_label_12.png', 'id_98_label_21.png', 'id_1261_label_15.png', 'id_123_label_6.png', 'id_2666_label_17.png', 'id_255_label_16.png', 'id_3123_label_22.png', 'id_2_label_1.png', 'id_1830_label_19.png', 'id_581_label_11.png', 'id_3254_label_3.png', 'id_1546_label_17.png', 'id_2586_label_5.png', 'id_2772_label_14.png', 'id_2185_label_1.png', 'id_13007_label_2.png', 'id_2474_label_2.png', 'id_330_label_25.png', 'id_2113_label_21.png', 'id_2539_label_10.png', 'id_2511_label_24.png', 'id_1318_label_15.png', 'id_691_label_10.png', 'id_1192_label_8.png', 'id_2033_label_9.png', 'id_1085_label_11.png', 'id_1074_label_5.png', 'id_3032_label_4.png', 'id_754_label_13.png', 'id_1500_label_22.png', 'id_1313_label_13.png', 'id_3293_label_23.png', 'id_2361_label_5.png', 'id_1994_label_17.png', 'id_2300_label_2.png', 'id_3183_label_24.png', 'id_538_label_17.png', 'id_218_label_25.png', 'id_2888_label_16.png', 'id_2048_label_16.png', 'id_3196_label_2.png', 'id_905_label_5.png', 'id_2430_label_11.png', 'id_422_label_15.png', 'id_2779_label_18.png', 'id_1991_label_16.png', 'id_2880_label_12.png', 'id_937_label_21.png', 'id_404_label_6.png', 'id_2226_label_21.png', 'id_3220_label_14.png', 'id_1198_label_11.png', 'id_463_label_8.png', 'id_1209_label_17.png', 'id_2330_label_17.png', 'id_2339_label_22.png', 'id_741_label_7.png', 'id_897_label_1.png', 'id_696_label_12.png', 'id_593_label_17.png', 'id_9_label_2.png', 'id_192_label_12.png', 'id_2378_label_13.png', 'id_1399_label_28.png', 'id_1138_label_9.png', 'id_3226_label_17.png', 'id_13227_label_2.png', 'id_441_label_25.png', 'id_2006_label_23.png', 'id_48_label_24.png', 'id_2088_label_8.png', 'id_385_label_25.png', 'id_2187_label_2.png', 'id_2421_label_7.png', 'id_3250_label_1.png', 'id_879_label_20.png', 'id_2798_label_27.png', 'id_221_label_27.png', 'id_2480_label_2.png', 'id_1551_label_20.png', 'id_2976_label_4.png', 'id_1200_label_12.png', 'id_3052_label_14.png', 'id_1107_label_22.png', 'id_296_label_8.png', 'id_1724_label_22.png', 'id_584_label_12.png', 'id_3147_label_2.png', 'id_1638_label_7.png', 'id_872_label_16.png', 'id_363_label_14.png', 'id_1862_label_7.png', 'id_481_label_17.png', 'id_2407_label_28.png', 'id_12_label_2.png', 'id_13003_label_2.png', 'id_3245_label_27.png', 'id_2785_label_21.png', 'id_16_label_2.png', 'id_3265_label_9.png', 'id_1424_label_12.png', 'id_3316_label_6.png', 'id_3234_label_21.png', 'id_767_label_20.png', 'id_2589_label_7.png', 'id_837_label_27.png', 'id_248_label_12.png', 'id_2490_label_13.png', 'id_2509_label_23.png', 'id_1956_label_26.png', 'id_2583_label_4.png', 'id_718_label_23.png', 'id_2921_label_2.png', 'id_2762_label_9.png', 'id_2603_label_14.png', 'id_2628_label_26.png', 'id_2025_label_2.png', 'id_198_label_15.png', 'id_3125_label_23.png', 'id_13_label_2.png', 'id_1125_label_3.png', 'id_940_label_22.png', 'id_2262_label_11.png', 'id_1815_label_12.png', 'id_2246_label_3.png', 'id_1840_label_24.png', 'id_2475_label_2.png', 'id_2104_label_16.png', 'id_1378_label_17.png', 'id_81_label_13.png', 'id_662_label_23.png', 'id_3180_label_22.png', 'id_1803_label_6.png', 'id_2926_label_2.png', 'id_1459_label_2.png', 'id_1347_label_2.png', 'id_210_label_21.png', 'id_2933_label_11.png', 'id_111_label_28.png', 'id_1397_label_27.png', 'id_1279_label_24.png', 'id_1865_label_9.png', 'id_2192_label_4.png', 'id_2032_label_2.png', 'id_669_label_27.png', 'id_133_label_11.png', 'id_997_label_23.png', 'id_2776_label_16.png', 'id_958_label_3.png', 'id_674_label_1.png', 'id_2479_label_2.png', 'id_860_label_10.png', 'id_3029_label_3.png', 'id_2609_label_17.png', 'id_1540_label_14.png', 'id_2972_label_2.png', 'id_2489_label_13.png', 'id_3080_label_28.png', 'id_54_label_27.png', 'id_3094_label_7.png', 'id_740_label_6.png', 'id_3146_label_2.png', 'id_2425_label_9.png', 'id_153_label_21.png', 'id_2031_label_2.png', 'id_1461_label_3.png', 'id_726_label_27.png', 'id_487_label_20.png', 'id_2090_label_9.png', 'id_3291_label_22.png', 'id_2949_label_19.png', 'id_1804_label_6.png', 'id_2913_label_1.png', 'id_733_label_3.png', 'id_2827_label_14.png', 'id_1813_label_11.png', 'id_1577_label_5.png', 'id_603_label_22.png', 'id_557_label_27.png', 'id_491_label_22.png', 'id_2749_label_3.png', 'id_2485_label_11.png', 'id_505_label_1.png', 'id_1026_label_9.png', 'id_1243_label_6.png', 'id_3092_label_6.png', 'id_1867_label_10.png', 'id_355_label_10.png', 'id_2050_label_17.png', 'id_2936_label_12.png', 'id_375_label_20.png', 'id_2140_label_6.png', 'id_2924_label_2.png', 'id_3150_label_2.png', 'id_1182_label_3.png', 'id_2363_label_6.png', 'id_2344_label_24.png', 'id_3005_label_19.png', 'id_13226_label_2.png', 'id_2718_label_15.png', 'id_2299_label_2.png', 'id_1631_label_4.png', 'id_2742_label_27.png', 'id_2889_label_17.png', 'id_106_label_25.png', 'id_1566_label_27.png', 'id_438_label_23.png', 'id_1828_label_18.png', 'id_552_label_24.png', 'id_1202_label_13.png', 'id_1061_label_27.png', 'id_1985_label_13.png', 'id_415_label_12.png', 'id_380_label_22.png', 'id_58_label_1.png', 'id_13225_label_2.png', 'id_374_label_19.png', 'id_2222_label_19.png', 'id_694_label_11.png', 'id_1501_label_23.png', 'id_1525_label_7.png', 'id_1005_label_27.png', 'id_2811_label_6.png', 'id_2516_label_26.png', 'id_95_label_20.png', 'id_1785_label_25.png', 'id_3114_label_17.png', 'id_37_label_19.png', 'id_1333_label_23.png', 'id_990_label_19.png', 'id_2032_label_8.png', 'id_776_label_24.png', 'id_2178_label_25.png', 'id_3307_label_2.png', 'id_687_label_8.png', 'id_910_label_7.png', 'id_531_label_14.png', 'id_3193_label_1.png', 'id_467_label_10.png', 'id_13232_label_2.png', 'id_2473_label_2.png', 'id_2030_label_2.png', 'id_798_label_7.png', 'id_3064_label_20.png', 'id_2964_label_26.png', 'id_1425_label_13.png', 'id_124_label_6.png', 'id_8_label_4.png', 'id_290_label_5.png', 'id_1396_label_26.png', 'id_299_label_10.png', 'id_894_label_27.png', 'id_2036_label_10.png', 'id_1975_label_8.png', 'id_791_label_4.png', 'id_479_label_16.png', 'id_3145_label_2.png', 'id_1335_label_24.png', 'id_292_label_6.png', 'id_2855_label_28.png', 'id_263_label_20.png', 'id_1836_label_22.png', 'id_991_label_20.png', 'id_2626_label_25.png', 'id_1681_label_1.png', 'id_2138_label_5.png', 'id_3241_label_25.png', 'id_2514_label_25.png', 'id_1732_label_26.png', 'id_1027_label_10.png', 'id_2392_label_20.png', 'id_140_label_14.png', 'id_3076_label_26.png', 'id_3010_label_21.png', 'id_3233_label_21.png', 'id_2922_label_2.png', 'id_1132_label_2.png', 'id_939_label_22.png', 'id_2188_label_2.png', 'id_1129_label_2.png', 'id_474_label_13.png', 'id_1214_label_19.png', 'id_843_label_2.png', 'id_1693_label_7.png', 'id_1664_label_20.png', 'id_2457_label_25.png', 'id_820_label_18.png', 'id_976_label_12.png', 'id_2819_label_10.png', 'id_888_label_24.png', 'id_3312_label_4.png', 'id_807_label_12.png', 'id_3219_label_14.png', 'id_2077_label_3.png', 'id_436_label_22.png', 'id_2069_label_27.png', 'id_929_label_17.png', 'id_3267_label_10.png', 'id_402_label_5.png', 'id_3037_label_7.png', 'id_2927_label_2.png', 'id_440_label_24.png', 'id_2029_label_2.png', 'id_1548_label_18.png', 'id_1642_label_9.png', 'id_3031_label_4.png', 'id_2697_label_5.png', 'id_3007_label_20.png', 'id_2831_label_16.png', 'id_2475_label_6.png', 'id_1655_label_16.png', 'id_2476_label_2.png', 'id_3195_label_2.png', 'id_68_label_6.png', 'id_760_label_16.png', 'id_931_label_18.png', 'id_445_label_27.png', 'id_2102_label_15.png', 'id_1250_label_9.png', 'id_2950_label_19.png', 'id_10_label_2.png', 'id_2643_label_6.png', 'id_533_label_15.png', 'id_801_label_9.png', 'id_1134_label_2.png', 'id_1423_label_12.png', 'id_950_label_27.png', 'id_3148_label_2.png', 'id_182_label_7.png', 'id_378_label_21.png', 'id_1626_label_1.png', 'id_2426_label_9.png', 'id_731_label_2.png', 'id_1069_label_3.png', 'id_1739_label_2.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path2 = \"/content/data/datasets/mohammad2012191/arabic-chars/versions/3/chars images/id_1989_label_15.png\"\n",
        "print(path2.split(\"/\")[-1].split(\"_\")[3].split(\".\")[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnvqLJT5XX7i",
        "outputId": "3be7fd09-ad3a-4020-cb42-79c176347c40"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UsLqWiZ2gdt"
      },
      "source": [
        "# TASK 1: Complete the following code to build a custom dataset class for the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T06:30:05.649942Z",
          "iopub.status.busy": "2025-02-18T06:30:05.649623Z",
          "iopub.status.idle": "2025-02-18T06:30:05.657730Z",
          "shell.execute_reply": "2025-02-18T06:30:05.656457Z",
          "shell.execute_reply.started": "2025-02-18T06:30:05.649915Z"
        },
        "id": "GOkfhgZpxLK0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ArabicHandwritingDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_paths =  glob.glob(f\"{root_dir}/*.png\")########################### TO DO: Use glob to extract images paths from root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image =  Image.open(img_path)########################### TO DO: Write a line to load the image from img_path\n",
        "        label = int(img_path.split(\"/\")[-1].split(\"_\")[3].split(\".\")[0])########################### TO DO: Write a line to extract the label from img_path\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T06:30:05.660017Z",
          "iopub.status.busy": "2025-02-18T06:30:05.659526Z",
          "iopub.status.idle": "2025-02-18T06:30:05.690778Z",
          "shell.execute_reply": "2025-02-18T06:30:05.689570Z",
          "shell.execute_reply.started": "2025-02-18T06:30:05.659980Z"
        },
        "trusted": true,
        "id": "hYZdiQhyJurc",
        "outputId": "ad063567-84af-4709-8442-8f0cadd2a5e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 668\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.Resize((32,32)), ########################### TO DO: Resize to 32*32\n",
        "    transforms.ToTensor(),########################### TO DO: Convert to Tensor\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),########################### TO DO: Normalize using Imagenet mean and std\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = ArabicHandwritingDataset(root_dir=os.path.join(dataset_path,\"chars images\"), transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)########################### TO DO\n",
        "print(f\"Total images: {len(train_dataset)}\")\n",
        "\n",
        "\n",
        "\n",
        "transformed_query_image = raw_query_image.transform########################### TO DO: Apply the transform to raw_query_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlsU08eB1RPQ"
      },
      "source": [
        "# TASK 2: Use EfficientNet B3 to extract features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Otiq1WbJurd"
      },
      "source": [
        "##### import using:\n",
        "from torchvision.models import efficientnet_b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXqoQqeFJurd"
      },
      "source": [
        "### Extract Features for the dataset images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "k_R5kalRJurd"
      },
      "outputs": [],
      "source": [
        "# TO DO: Load pretrained efficientnet_b3 model\n",
        "\n",
        "# TO DO: Use efficientnet_b3 to extract features\n",
        "\n",
        "# TO DO: Convert features to numpy and concatenate them.\n",
        "\n",
        "# TO DO: Concatenate the raw images tensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load pretrained EfficientNetV2-S model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "efficientnet = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
        "efficientnet.eval().to(device)\n",
        "\n",
        "\n",
        "# input (image)  --> Conv layers (Feature Extraction) --> Fully Connected Layers (classification) -->\n",
        "\n",
        "\n",
        "# Modify EfficientNetV2-S to extract features before classification\n",
        "def extract_features(image_tensor):\n",
        "    image_tensor = image_tensor.to(device)\n",
        "    with torch.no_grad():\n",
        "        features = efficientnet.features(image_tensor)  # Extract features\n",
        "        features = torch.flatten(features, start_dim=1)  # Flatten before passing to classifier\n",
        "        # features = efficientnet.classifier[0](features)  # Pass through first layer of classifier\n",
        "    return features\n",
        "\n",
        "# Extract features for all images\n",
        "all_features = []\n",
        "image_tensors = []\n",
        "\n",
        "\n",
        "for images, _ in tqdm(train_loader):\n",
        "    image_tensors.append(images)\n",
        "    features = extract_features(images)\n",
        "    all_features.append(features.cpu())\n",
        "\n",
        "\n",
        "print(f\"all_features shapes {torch.cat(all_features).shape}\")\n",
        "\n",
        "# Convert to numpy\n",
        "all_features = torch.cat(all_features).numpy()\n",
        "image_tensors = torch.cat(image_tensors)\n",
        "\n",
        "\n",
        "print(f\"image_tensors shapes {image_tensors.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9T9eb7lbUVV",
        "outputId": "f6b4e0da-0e58-4649-9aa8-9eddfcf01571"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 668/668 [00:23<00:00, 27.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_features shapes torch.Size([668, 1536])\n",
            "image_tensors shapes torch.Size([668, 3, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrnv6Q1OJurd"
      },
      "source": [
        "### Extract Features for the query image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T06:30:13.453921Z",
          "iopub.status.busy": "2025-02-18T06:30:13.453350Z",
          "iopub.status.idle": "2025-02-18T06:30:13.492599Z",
          "shell.execute_reply": "2025-02-18T06:30:13.491384Z",
          "shell.execute_reply.started": "2025-02-18T06:30:13.453871Z"
        },
        "trusted": true,
        "id": "lYyANrzZJure",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "20cb6969-d1a4-4969-e4e3-397d53489e88"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'to'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-faa9d799dcf7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### TO DO: Fix the error in this line so it extracts features from the query image successfully\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery_image_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_query_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-435930c3a5cd>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(image_tensor)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Modify EfficientNetV2-S to extract features before classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefficientnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "### TO DO: Fix the error in this line so it extracts features from the query image successfully\n",
        "query_image_feats = extract_features(transformed_query_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33H1GIb41uG0"
      },
      "source": [
        "# TASK 3: Get the indices of top 5 images using Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T06:30:13.494220Z",
          "iopub.status.busy": "2025-02-18T06:30:13.493875Z",
          "iopub.status.idle": "2025-02-18T06:30:13.512319Z",
          "shell.execute_reply": "2025-02-18T06:30:13.510810Z",
          "shell.execute_reply.started": "2025-02-18T06:30:13.494190Z"
        },
        "id": "-0gHEjRi10FQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Select a random query image\n",
        "random_idx = random.randint(0, len(pca_features) - 1)\n",
        "query_image = image_tensors[random_idx]\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarities = cosine_similarity(, query_features.reshape(1, -1)).reshape(-1)\n",
        "\n",
        "# Get top 5 similar images\n",
        "top_indices = similarities.argsort()[-6:-1][::-1]  # Ignore query itself\n",
        "\n",
        "# Display query & retrieved images with similarity scores\n",
        "fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
        "\n",
        "# Query image\n",
        "axes[0].imshow(query_image.permute(1, 2, 0).numpy())\n",
        "axes[0].set_title(\"Query Image\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "# Retrieved similar images with similarity scores\n",
        "for i, idx in enumerate(top_indices):\n",
        "    image = image_tensors[idx]\n",
        "    similarity_score = similarities[idx]  # Get similarity score\n",
        "    axes[i + 1].imshow(image.permute(1, 2, 0).numpy())\n",
        "    axes[i + 1].set_title(f\"Similar {i+1}\\n{100*similarity_score:.2f}%\")  # Show similarity score\n",
        "    axes[i + 1].axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viNWPZsH12fG"
      },
      "source": [
        "# TASK 4: Fix the errors in the following code to plot the most 5 similar images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nwlMW-518q4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Display query & retrieved images with similarity scores\n",
        "fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
        "\n",
        "# Query image\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title(\"Query Image\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "# Retrieved similar images with similarity scores\n",
        "for i, idx in enumerate(top_indices):\n",
        "    image = image_tensors[idx]\n",
        "    similarity_score = similarities[idx]  # Get similarity score\n",
        "    axes[i + 1].imshow(image))\n",
        "    axes[i + 1].set_title(f\"Similar {i+1}\\n{100*similarity_score:.2f}%\")  # Show similarity score\n",
        "    axes[i + 1].axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jhRQLRJ1-y5"
      },
      "source": [
        "# Bonus Task:  Get the indices of the most 5 *dissimilar* images and plot them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "n2XW0YfzJure"
      },
      "outputs": [],
      "source": [
        "### TO DO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6683776,
          "sourceId": 10779845,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6683904,
          "sourceId": 10779858,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30886,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}